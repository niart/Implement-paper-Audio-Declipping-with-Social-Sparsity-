{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change backbone to MobileNet\n",
    "### First, setup libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "\n",
    "# A module to print a model summary (outputs shape, number of parameters, ...)\n",
    "import torchsummary\n",
    "\n",
    "# TensorBoard for visualization\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#from tensorboardX import SummaryWriter\n",
    "tensorboard = SummaryWriter()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "# plt.rcParams['text.usetex'] = True  # Render Matplotlib text with Tex\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data to be used\n",
    "DATASET = \"./datasets/dataset_3+8bags_3var3sc/\"\n",
    "\n",
    "\n",
    "class TraversabilityDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class to represent our dataset\n",
    "    It includes data and information about the data\n",
    "\n",
    "    Args:\n",
    "        Dataset (class): Abstract class which represents a dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, traversal_costs_file, images_directory,\n",
    "                 transform=None):\n",
    "        \"\"\"Constructor of the class\n",
    "\n",
    "        Args:\n",
    "            traversal_costs_file (string): Path to the csv file which contains\n",
    "            images index and their associated traversal cost\n",
    "            images_directory (string): Directory with all the images\n",
    "            transform (callable, optional): Transforms to be applied on a\n",
    "            sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Read the csv file\n",
    "        self.traversal_costs_frame = pd.read_csv(traversal_costs_file)\n",
    "        \n",
    "        # Initialize the name of the images directory\n",
    "        self.images_directory = images_directory\n",
    "        \n",
    "        # Initialize the transforms\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples\n",
    "        \"\"\"\n",
    "        # Count the number of files in the image directory\n",
    "        # return len(os.listdir(self.images_directory))\n",
    "        return len(self.traversal_costs_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Allow to access a sample by its index\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of a sample\n",
    "\n",
    "        Returns:\n",
    "            list: Sample at index idx\n",
    "            ([image, traversal_cost])\n",
    "        \"\"\"\n",
    "        # Get the image name at index idx\n",
    "        image_name = os.path.join(self.images_directory,\n",
    "                                  self.traversal_costs_frame.loc[idx, \"image_id\"])\n",
    "        \n",
    "        # Read the image\n",
    "        image = Image.open(image_name)\n",
    "        \n",
    "        # Eventually apply transforms to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get the corresponding traversal cost\n",
    "        traversal_cost = self.traversal_costs_frame.loc[idx, \"traversal_cost\"]\n",
    "        \n",
    "        # Get the corresponding traversability label\n",
    "        traversability_label = self.traversal_costs_frame.loc[idx, \"traversability_label\"]\n",
    "\n",
    "        return image, traversal_cost, traversability_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(images_directory, traversal_costs_file):\n",
    "    transform = transforms.Compose([\n",
    "        # Reduce the size of the images\n",
    "        # (if size is an int, the smaller edge of the\n",
    "        # image will be matched to this number and the ration is kept)\n",
    "        transforms.Resize((70, 210)),\n",
    "\n",
    "        # Convert a PIL Image or numpy.ndarray to tensor\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Create a Dataset for training\n",
    "    dataset = TraversabilityDataset(\n",
    "        traversal_costs_file=DATASET+traversal_costs_file,\n",
    "        images_directory=DATASET+images_directory,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=12,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    cnt = 0\n",
    "    first_moment = torch.empty(3)\n",
    "    second_moment = torch.empty(3)\n",
    "\n",
    "    for images, traversal_costs, traversability_labels in loader:\n",
    "        b, c, h, w = images.shape\n",
    "        nb_pixels = b * h * w\n",
    "        sum_ = torch.sum(images, dim=[0, 2, 3])\n",
    "        sum_of_square = torch.sum(images ** 2, dim=[0, 2, 3])\n",
    "        first_moment = (cnt * first_moment + sum_) / (cnt + nb_pixels)\n",
    "        second_moment = (cnt * second_moment + sum_of_square) / (cnt + nb_pixels)\n",
    "        cnt += nb_pixels\n",
    "\n",
    "    mean = first_moment\n",
    "    std = torch.sqrt(second_moment - first_moment ** 2)\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3426, 0.3569, 0.2914]) tensor([0.1363, 0.1248, 0.1302])\n"
     ]
    }
   ],
   "source": [
    "mean, std = compute_mean_std(\"images_train\", \"traversal_costs_train.csv\")\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose several transforms together to be applied to training data\n",
    "# (Note that transforms are not applied yet)\n",
    "train_transform = transforms.Compose([\n",
    "    # Reduce the size of the images\n",
    "    # (if size is an int, the smaller edge of the\n",
    "    # image will be matched to this number and the ration is kept)\n",
    "    # transforms.Resize(100),\n",
    "    transforms.Resize((70, 210)),\n",
    "    \n",
    "    # Perform horizontal flip of the image with a probability of 0.5\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    \n",
    "    # Modify the brightness and the contrast of the image\n",
    "    transforms.ColorJitter(contrast=0.5, brightness=0.5),\n",
    "    \n",
    "    # Convert a PIL Image or numpy.ndarray to tensor\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Add some random gaussian noise to the image\n",
    "    transforms.Lambda(lambda x: x + (0.001**0.5)*torch.randn(x.shape)),\n",
    "    \n",
    "    # Normalize a tensor image with pre-computed mean and standard deviation\n",
    "    # (based on the data used to train the model(s))\n",
    "    # (be careful, it only works on torch.*Tensor)\n",
    "    transforms.Normalize(\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "        # mean=[0.485, 0.456, 0.406],\n",
    "        # std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define a different set of transforms testing\n",
    "# (for instance we do not need to flip the image)\n",
    "test_transform = transforms.Compose([\n",
    "    # transforms.Resize(100),\n",
    "    transforms.Resize((70, 210)),\n",
    "    # transforms.Grayscale(),\n",
    "    # transforms.CenterCrop(100),\n",
    "    # transforms.RandomCrop(100),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Mean and standard deviation were pre-computed on the training data\n",
    "    # (on the ImageNet dataset)\n",
    "    transforms.Normalize(\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "        # mean=[0.485, 0.456, 0.406],\n",
    "        # std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "# # Create a Dataset instance for our training data\n",
    "# data = TraversabilityDataset(\n",
    "#     traversal_costs_file=DATASET+\"traversal_costs.csv\",\n",
    "#     images_directory=DATASET+\"images\",\n",
    "#     transform=train_transform\n",
    "# )\n",
    "\n",
    "# # Split our training dataset into a training dataset and a validation dataset\n",
    "# train_set, val_set, test_set = random_split(data, [0.8, 0.1, 0.1])\n",
    "\n",
    "\n",
    "# Create a Dataset for training\n",
    "train_set = TraversabilityDataset(\n",
    "    traversal_costs_file=DATASET+\"traversal_costs_train.csv\",\n",
    "    images_directory=DATASET+\"images_train\",\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# Create a Dataset for validation\n",
    "val_set = TraversabilityDataset(\n",
    "    traversal_costs_file=DATASET+\"traversal_costs_train.csv\",\n",
    "    images_directory=DATASET+\"images_train\",\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "# Create a Dataset for testin\n",
    "test_set = TraversabilityDataset(\n",
    "    traversal_costs_file=DATASET+\"traversal_costs_test.csv\",\n",
    "    images_directory=DATASET+\"images_test\",\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "# Set the train dataset size\n",
    "# 70% of the total data is used for training, 15% for validation\n",
    "# and 15% for testing\n",
    "train_size = 70/(100-15)\n",
    "\n",
    "# Splits train data indices into train and validation data indices\n",
    "train_indices, val_indices = train_test_split(range(len(train_set)), train_size=train_size)\n",
    "\n",
    "# Extract the corresponding subsets of the train dataset\n",
    "train_set = Subset(train_set, train_indices)\n",
    "val_set = Subset(val_set, val_indices)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Combine a dataset and a sampler, and provide an iterable over the dataset\n",
    "# (setting shuffle argument to True calls a RandomSampler, and avoids to\n",
    "# have to create a Sampler object)\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=12,  # Asynchronous data loading and augmentation\n",
    "    pin_memory=True,  # Increase the transferring speed of the data to the GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # SequentialSampler\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of samples per split\n",
    "NB_TRAIN = len(train_set)\n",
    "NB_VAL = len(val_set)\n",
    "NB_TEST = len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0 % of the data for training (6239 samples)\n",
      "15.0 % of the data for validation (1337 samples)\n",
      "15.0 % of the data for testing (1338 samples)\n"
     ]
    }
   ],
   "source": [
    "# Display the splits ratio\n",
    "NB_SAMPLES = NB_TRAIN + NB_VAL + NB_TEST\n",
    "\n",
    "print(f\"{np.round(NB_TRAIN/NB_SAMPLES*100)} % of the data for training ({NB_TRAIN} samples)\")\n",
    "print(f\"{np.round(NB_VAL/NB_SAMPLES*100)} % of the data for validation ({NB_VAL} samples)\")\n",
    "print(f\"{np.round(NB_TEST/NB_SAMPLES*100)} % of the data for testing ({NB_TEST} samples)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a GPU if available\n",
    "# device = \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model design and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): ConvNormActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (2): ConvNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvNormActivation(\n",
      "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): ConvNormActivation(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Load the pre-trained MobileNetV3 model\n",
    "model = models.mobilenet_v3_small(pretrained=True).to(device=device)\n",
    "\n",
    "# Replace the last layer by a fully-connected one with 1 output\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, 1).to(device=device)\n",
    "\n",
    "# Initialize the last layer using Xavier initialization\n",
    "nn.init.xavier_uniform_(model.classifier[3].weight)\n",
    "\n",
    "# Get all the parameters except for the last layer's weight and bias\n",
    "base_params = [param for name, param in model.named_parameters()\n",
    "               if name != \"classifier.3.weight\" and name != \"classifier.3.bias\"]\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.40batch/s, batch_loss=0.0722]\n",
      "Epoch 0 [val]: 100%|██████████| 42/42 [00:00<00:00, 63.39batch/s, batch_loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.3452826154155609\n",
      "Validation loss:  0.16697187987821444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.13batch/s, batch_loss=0.0845]\n",
      "Epoch 1 [val]: 100%|██████████| 42/42 [00:00<00:00, 64.95batch/s, batch_loss=0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.10811623915647849\n",
      "Validation loss:  0.19179782785830043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.75batch/s, batch_loss=0.0905]\n",
      "Epoch 2 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.99batch/s, batch_loss=0.219] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.0837370614401805\n",
      "Validation loss:  0.15391526087408974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.90batch/s, batch_loss=0.0836]\n",
      "Epoch 3 [val]: 100%|██████████| 42/42 [00:00<00:00, 68.08batch/s, batch_loss=0.151] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.07640078079241973\n",
      "Validation loss:  0.14956007383409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.56batch/s, batch_loss=0.0925]\n",
      "Epoch 4 [val]: 100%|██████████| 42/42 [00:00<00:00, 66.05batch/s, batch_loss=0.126] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.06730217789419186\n",
      "Validation loss:  0.11038016971378099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.37batch/s, batch_loss=0.078] \n",
      "Epoch 5 [val]: 100%|██████████| 42/42 [00:00<00:00, 64.28batch/s, batch_loss=0.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.06501643294707322\n",
      "Validation loss:  0.09098664431699685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.48batch/s, batch_loss=0.105] \n",
      "Epoch 6 [val]: 100%|██████████| 42/42 [00:00<00:00, 67.05batch/s, batch_loss=0.116] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.06319869158741756\n",
      "Validation loss:  0.10950578926574617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.40batch/s, batch_loss=0.0735]\n",
      "Epoch 7 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.63batch/s, batch_loss=0.0867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.059901297178405984\n",
      "Validation loss:  0.09927792545585405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.36batch/s, batch_loss=0.0437]\n",
      "Epoch 8 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.88batch/s, batch_loss=0.0747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.056979312929205404\n",
      "Validation loss:  0.09180796882581144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [train]: 100%|██████████| 195/195 [00:04<00:00, 48.05batch/s, batch_loss=0.0339]\n",
      "Epoch 9 [val]: 100%|██████████| 42/42 [00:00<00:00, 67.83batch/s, batch_loss=0.0669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05796153778449083\n",
      "Validation loss:  0.10346992375950019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.08batch/s, batch_loss=0.0602]\n",
      "Epoch 10 [val]: 100%|██████████| 42/42 [00:00<00:00, 66.16batch/s, batch_loss=0.145] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05587520475188891\n",
      "Validation loss:  0.09177263932568687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.18batch/s, batch_loss=0.0416]\n",
      "Epoch 11 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.68batch/s, batch_loss=0.0861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.054947382546006104\n",
      "Validation loss:  0.10125829918043953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.27batch/s, batch_loss=0.0458]\n",
      "Epoch 12 [val]: 100%|██████████| 42/42 [00:00<00:00, 66.56batch/s, batch_loss=0.108]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.0530802455277015\n",
      "Validation loss:  0.08801435918680259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.37batch/s, batch_loss=0.041] \n",
      "Epoch 13 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.82batch/s, batch_loss=0.0814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05471583743317005\n",
      "Validation loss:  0.07971575226457346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.54batch/s, batch_loss=0.105] \n",
      "Epoch 14 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.25batch/s, batch_loss=0.0627] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.053594062391381994\n",
      "Validation loss:  0.07981278747320175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.08batch/s, batch_loss=0.0586]\n",
      "Epoch 15 [val]: 100%|██████████| 42/42 [00:00<00:00, 66.82batch/s, batch_loss=0.0977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05114475630987913\n",
      "Validation loss:  0.07731329552119687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [train]: 100%|██████████| 195/195 [00:04<00:00, 48.15batch/s, batch_loss=0.1]   \n",
      "Epoch 16 [val]: 100%|██████████| 42/42 [00:00<00:00, 66.46batch/s, batch_loss=0.126] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05101254899532367\n",
      "Validation loss:  0.08036852823126883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.58batch/s, batch_loss=0.0442]\n",
      "Epoch 17 [val]: 100%|██████████| 42/42 [00:00<00:00, 67.39batch/s, batch_loss=0.0572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05220558724533289\n",
      "Validation loss:  0.07700406901893161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.86batch/s, batch_loss=0.0331]\n",
      "Epoch 18 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.85batch/s, batch_loss=0.0374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05045859769750864\n",
      "Validation loss:  0.0758516565852222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.75batch/s, batch_loss=0.0405]\n",
      "Epoch 19 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.60batch/s, batch_loss=0.0699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05132328289059492\n",
      "Validation loss:  0.07610466756990977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.49batch/s, batch_loss=0.0487]\n",
      "Epoch 20 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.71batch/s, batch_loss=0.0891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04996244597893495\n",
      "Validation loss:  0.07681740119698502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.40batch/s, batch_loss=0.0735]\n",
      "Epoch 21 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.41batch/s, batch_loss=0.0875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04919112038153869\n",
      "Validation loss:  0.08161598027107261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.71batch/s, batch_loss=0.0574]\n",
      "Epoch 22 [val]: 100%|██████████| 42/42 [00:00<00:00, 64.12batch/s, batch_loss=0.0862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04814010012226227\n",
      "Validation loss:  0.06997562998107501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.63batch/s, batch_loss=0.045] \n",
      "Epoch 23 [val]: 100%|██████████| 42/42 [00:00<00:00, 66.59batch/s, batch_loss=0.0514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04724120118488104\n",
      "Validation loss:  0.07184718504902862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.22batch/s, batch_loss=0.0284]\n",
      "Epoch 24 [val]: 100%|██████████| 42/42 [00:00<00:00, 67.31batch/s, batch_loss=0.0712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04664916613449653\n",
      "Validation loss:  0.07478744492289566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.15batch/s, batch_loss=0.0811]\n",
      "Epoch 25 [val]: 100%|██████████| 42/42 [00:00<00:00, 68.41batch/s, batch_loss=0.0971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.048127981638297054\n",
      "Validation loss:  0.07166963212546848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.72batch/s, batch_loss=0.0552]\n",
      "Epoch 26 [val]: 100%|██████████| 42/42 [00:00<00:00, 68.12batch/s, batch_loss=0.0646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.047497416459597076\n",
      "Validation loss:  0.0663031159589688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 [train]: 100%|██████████| 195/195 [00:04<00:00, 48.66batch/s, batch_loss=0.0331]\n",
      "Epoch 27 [val]: 100%|██████████| 42/42 [00:00<00:00, 67.25batch/s, batch_loss=0.0631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04753155640493601\n",
      "Validation loss:  0.07393561427791913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.92batch/s, batch_loss=0.0406]\n",
      "Epoch 28 [val]: 100%|██████████| 42/42 [00:00<00:00, 63.50batch/s, batch_loss=0.065] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04616622818777194\n",
      "Validation loss:  0.06686891979050069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.13batch/s, batch_loss=0.0203]\n",
      "Epoch 29 [val]: 100%|██████████| 42/42 [00:00<00:00, 64.85batch/s, batch_loss=0.0827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04695799306799204\n",
      "Validation loss:  0.06633973148252283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.19batch/s, batch_loss=0.0292]\n",
      "Epoch 30 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.34batch/s, batch_loss=0.083] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.046297333332208486\n",
      "Validation loss:  0.07617957672725122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.31batch/s, batch_loss=0.0642]\n",
      "Epoch 31 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.96batch/s, batch_loss=0.076] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04579898407443976\n",
      "Validation loss:  0.08084605394729547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.27batch/s, batch_loss=0.0398]\n",
      "Epoch 32 [val]: 100%|██████████| 42/42 [00:00<00:00, 66.28batch/s, batch_loss=0.0878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04568530284823515\n",
      "Validation loss:  0.07574940703454472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.36batch/s, batch_loss=0.0313]\n",
      "Epoch 33 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.81batch/s, batch_loss=0.0765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.045093072201005925\n",
      "Validation loss:  0.08153960977991422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34 [train]: 100%|██████████| 195/195 [00:04<00:00, 45.19batch/s, batch_loss=0.0523]\n",
      "Epoch 34 [val]: 100%|██████████| 42/42 [00:00<00:00, 65.47batch/s, batch_loss=0.0498] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04592375547075883\n",
      "Validation loss:  0.07230103556953725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.45batch/s, batch_loss=0.0264]\n",
      "Epoch 35 [val]: 100%|██████████| 42/42 [00:00<00:00, 62.14batch/s, batch_loss=0.0307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.045306002931335035\n",
      "Validation loss:  0.06939083541787806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36 [train]: 100%|██████████| 195/195 [00:04<00:00, 45.94batch/s, batch_loss=0.0357]\n",
      "Epoch 36 [val]: 100%|██████████| 42/42 [00:00<00:00, 63.04batch/s, batch_loss=0.104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.045178172469903265\n",
      "Validation loss:  0.06690380132446687\n",
      "Early stopping at epoch 36\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer, with a greater learning rate for the new last layer\n",
    "optimizer = optim.SGD([\n",
    "    {\"params\": base_params},\n",
    "    {\"params\": model.classifier[3].parameters(), \"lr\": 1e-3},\n",
    "],\n",
    "    lr=1e-3, momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "# An epoch is one complete pass of the training dataset through the network\n",
    "NB_EPOCHS = 100\n",
    "\n",
    "# Number of epochs we wait for the loss to decrease before stopping\n",
    "# the training process early\n",
    "patience = 10\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "loss_values = torch.zeros(2, NB_EPOCHS)\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    \n",
    "    # Training\n",
    "    train_loss = 0.\n",
    "    \n",
    "    # Configure the model for training\n",
    "    # (good practice, only necessary if the model operates differently for\n",
    "    # training and validation)\n",
    "    model.train()\n",
    "    \n",
    "    # Add a progress bar\n",
    "    train_loader_pbar = tqdm(train_loader, unit=\"batch\")\n",
    "    \n",
    "    # Loop over the training batches\n",
    "    for images, traversal_costs, _ in train_loader_pbar:\n",
    "        \n",
    "        # Print the epoch and training mode\n",
    "        train_loader_pbar.set_description(f\"Epoch {epoch} [train]\")\n",
    "        \n",
    "        # Move images and traversal scores to GPU (if available)\n",
    "        images = images.to(device)\n",
    "        traversal_costs = traversal_costs.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        # Zero out gradients before each backpropagation pass, to avoid that\n",
    "        # they accumulate\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        predicted_traversal_costs = model(images)\n",
    "        \n",
    "        # Compute loss \n",
    "        loss = criterion(predicted_traversal_costs[:, 0], traversal_costs)\n",
    "        \n",
    "        # Print the batch loss next to the progress bar\n",
    "        train_loader_pbar.set_postfix(batch_loss=loss.item())\n",
    "        \n",
    "        # Perform backpropagation (update weights)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Adjust parameters based on gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate batch loss to average over the epoch\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    # Validation\n",
    "    val_loss = 0.\n",
    "    \n",
    "    # Configure the model for testing\n",
    "    # (turn off dropout layers, batchnorm layers, etc)\n",
    "    model.eval()\n",
    "    \n",
    "    # Add a progress bar\n",
    "    val_loader_pbar = tqdm(val_loader, unit=\"batch\")\n",
    "    \n",
    "    # Turn off gradients computation (the backward computational graph is built during\n",
    "    # the forward pass and weights are updated during the backward pass, here we avoid\n",
    "    # building the graph)\n",
    "    with torch.no_grad():\n",
    "        # Loop over the validation batches\n",
    "        for images, traversal_costs, _ in val_loader_pbar:\n",
    "\n",
    "            # Print the epoch and validation mode\n",
    "            val_loader_pbar.set_description(f\"Epoch {epoch} [val]\")\n",
    "\n",
    "            # Move images and traversal scores to GPU (if available)\n",
    "            images = images.to(device)\n",
    "            traversal_costs = traversal_costs.type(torch.FloatTensor).to(device)\n",
    "            \n",
    "            # Perform forward pass (only, no backpropagation)\n",
    "            predicted_traversal_costs = model(images)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predicted_traversal_costs[:, 0], traversal_costs)\n",
    "            # Print the batch loss next to the progress bar\n",
    "            val_loader_pbar.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "            # Accumulate batch loss to average over the epoch\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    \n",
    "    # Compute the losses\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(\"Train loss: \", train_loss)\n",
    "    print(\"Validation loss: \", val_loss)\n",
    "    \n",
    "    # Store the computed losses\n",
    "    loss_values[0, epoch] = train_loss\n",
    "    loss_values[1, epoch] = val_loss\n",
    "    \n",
    "    # Add the losses to TensorBoard\n",
    "    tensorboard.add_scalar(\"train_loss\", train_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val_loss\", val_loss, epoch)\n",
    "    \n",
    "    # Early stopping based on validation loss: stop the training if the\n",
    "    # loss has not improved for the last 5 epochs\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    elif epoch - best_epoch >= patience:\n",
    "        print(f'Early stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "# Close TensorBoard\n",
    "tensorboard.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['text.usetex'] = False\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss\n",
    "indices = loss_values[0] != 0\n",
    "train_losses = loss_values[0][indices]\n",
    "val_losses = loss_values[1][indices]\n",
    "\n",
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(val_losses, label=\"validation loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Losses (MSE)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.06200482259595691\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_loss = 0.\n",
    "\n",
    "# Configure the model for testing\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Loop over the testing batches\n",
    "    for images, traversal_costs, _ in test_loader:\n",
    "        \n",
    "        images = images.to(device)\n",
    "        traversal_costs = traversal_costs.to(device)\n",
    "        \n",
    "        # Perform forward pass\n",
    "        predicted_traversal_costs = model(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predicted_traversal_costs[:, 0], traversal_costs)\n",
    "        \n",
    "        # Accumulate batch loss to average of the entire testing set\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "# Compute the loss and accuracy\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "print(\"Test loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " tensor([[1.8398],\n",
      "        [1.2309],\n",
      "        [1.5391],\n",
      "        [1.8236],\n",
      "        [1.7630],\n",
      "        [1.5386],\n",
      "        [1.6536],\n",
      "        [1.6837],\n",
      "        [1.5471],\n",
      "        [1.6870],\n",
      "        [1.6941],\n",
      "        [1.3636],\n",
      "        [1.6383],\n",
      "        [1.8881],\n",
      "        [1.5181],\n",
      "        [1.9960],\n",
      "        [1.6259],\n",
      "        [1.6850],\n",
      "        [1.8601],\n",
      "        [1.3048],\n",
      "        [1.7856],\n",
      "        [1.5162],\n",
      "        [1.3262],\n",
      "        [1.6464],\n",
      "        [1.6397],\n",
      "        [1.3841]], device='cuda:0')\n",
      "Ground truth:\n",
      " tensor([1.4431, 1.8142, 1.4213, 1.8988, 1.6864, 1.0896, 1.3164, 1.1830, 1.9168,\n",
      "        1.0740, 1.7578, 1.9497, 1.1054, 1.4927, 2.1730, 1.6349, 1.9143, 1.6020,\n",
      "        1.8331, 2.0072, 1.7344, 1.1041, 1.8847, 1.4045, 0.9980, 2.2835, 1.8514,\n",
      "        1.1247, 2.0487, 1.2955, 1.9082, 1.9524], device='cuda:0',\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "images, traversal_costs, _ = next(iter(test_loader))\n",
    "\n",
    "images = images.to(device)\n",
    "traversal_costs = traversal_costs.to(device)\n",
    "\n",
    "predicted_traversal_scores = model(images)\n",
    "# predicted_traversal_scores = nn.Softmax(dim=1)(model(images))\n",
    "\n",
    "print(\"Output:\\n\", predicted_traversal_costs)\n",
    "print(\"Ground truth:\\n\", traversal_costs)\n",
    "\n",
    "# print(predicted_traversal_scores-traversal_scores)\n",
    "\n",
    "# predicted_traversal_scores = predicted_traversal_scores.to(\"cpu\").detach().numpy()\n",
    "# plt.hist(predicted_traversal_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "traversal_costs_train = []\n",
    "\n",
    "for _, score, _ in train_set:\n",
    "    traversal_costs_train.append(score)\n",
    "    \n",
    "plt.hist(traversal_costs_train, bins=10)\n",
    "plt.xlabel(\"Traversal Cost\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of Traversal Costs (Training Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters\n",
    "torch.save(model.state_dict(), \"mobilenet.params\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b41780248731413e84842f51b5f7cba9458d17cab322c1e6e009edd67950520d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

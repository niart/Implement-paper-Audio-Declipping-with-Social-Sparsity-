{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change backbone to MobileNet\n",
    "### First, setup libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "\n",
    "# A module to print a model summary (outputs shape, number of parameters, ...)\n",
    "import torchsummary\n",
    "\n",
    "# TensorBoard for visualization\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#from tensorboardX import SummaryWriter\n",
    "tensorboard = SummaryWriter()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "# plt.rcParams['text.usetex'] = True  # Render Matplotlib text with Tex\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data to be used\n",
    "DATASET = \"./datasets/dataset_3+8bags_3var3sc/\"\n",
    "\n",
    "\n",
    "class TraversabilityDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class to represent our dataset\n",
    "    It includes data and information about the data\n",
    "\n",
    "    Args:\n",
    "        Dataset (class): Abstract class which represents a dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, traversal_costs_file, images_directory,\n",
    "                 transform=None):\n",
    "        \"\"\"Constructor of the class\n",
    "\n",
    "        Args:\n",
    "            traversal_costs_file (string): Path to the csv file which contains\n",
    "            images index and their associated traversal cost\n",
    "            images_directory (string): Directory with all the images\n",
    "            transform (callable, optional): Transforms to be applied on a\n",
    "            sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Read the csv file\n",
    "        self.traversal_costs_frame = pd.read_csv(traversal_costs_file)\n",
    "        \n",
    "        # Initialize the name of the images directory\n",
    "        self.images_directory = images_directory\n",
    "        \n",
    "        # Initialize the transforms\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples\n",
    "        \"\"\"\n",
    "        # Count the number of files in the image directory\n",
    "        # return len(os.listdir(self.images_directory))\n",
    "        return len(self.traversal_costs_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Allow to access a sample by its index\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of a sample\n",
    "\n",
    "        Returns:\n",
    "            list: Sample at index idx\n",
    "            ([image, traversal_cost])\n",
    "        \"\"\"\n",
    "        # Get the image name at index idx\n",
    "        image_name = os.path.join(self.images_directory,\n",
    "                                  self.traversal_costs_frame.loc[idx, \"image_id\"])\n",
    "        \n",
    "        # Read the image\n",
    "        image = Image.open(image_name)\n",
    "        \n",
    "        # Eventually apply transforms to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get the corresponding traversal cost\n",
    "        traversal_cost = self.traversal_costs_frame.loc[idx, \"traversal_cost\"]\n",
    "        \n",
    "        # Get the corresponding traversability label\n",
    "        traversability_label = self.traversal_costs_frame.loc[idx, \"traversability_label\"]\n",
    "\n",
    "        return image, traversal_cost, traversability_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(images_directory, traversal_costs_file):\n",
    "    transform = transforms.Compose([\n",
    "        # Reduce the size of the images\n",
    "        # (if size is an int, the smaller edge of the\n",
    "        # image will be matched to this number and the ration is kept)\n",
    "        transforms.Resize((70, 210)),\n",
    "\n",
    "        # Convert a PIL Image or numpy.ndarray to tensor\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Create a Dataset for training\n",
    "    dataset = TraversabilityDataset(\n",
    "        traversal_costs_file=DATASET+traversal_costs_file,\n",
    "        images_directory=DATASET+images_directory,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=12,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    cnt = 0\n",
    "    first_moment = torch.empty(3)\n",
    "    second_moment = torch.empty(3)\n",
    "\n",
    "    for images, traversal_costs, traversability_labels in loader:\n",
    "        b, c, h, w = images.shape\n",
    "        nb_pixels = b * h * w\n",
    "        sum_ = torch.sum(images, dim=[0, 2, 3])\n",
    "        sum_of_square = torch.sum(images ** 2, dim=[0, 2, 3])\n",
    "        first_moment = (cnt * first_moment + sum_) / (cnt + nb_pixels)\n",
    "        second_moment = (cnt * second_moment + sum_of_square) / (cnt + nb_pixels)\n",
    "        cnt += nb_pixels\n",
    "\n",
    "    mean = first_moment\n",
    "    std = torch.sqrt(second_moment - first_moment ** 2)\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3426, 0.3569, 0.2914]) tensor([0.1363, 0.1248, 0.1302])\n"
     ]
    }
   ],
   "source": [
    "mean, std = compute_mean_std(\"images_train\", \"traversal_costs_train.csv\")\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose several transforms together to be applied to training data\n",
    "# (Note that transforms are not applied yet)\n",
    "train_transform = transforms.Compose([\n",
    "    # Reduce the size of the images\n",
    "    # (if size is an int, the smaller edge of the\n",
    "    # image will be matched to this number and the ration is kept)\n",
    "    # transforms.Resize(100),\n",
    "    transforms.Resize((70, 210)),\n",
    "    \n",
    "    # Perform horizontal flip of the image with a probability of 0.5\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    \n",
    "    # Modify the brightness and the contrast of the image\n",
    "    transforms.ColorJitter(contrast=0.5, brightness=0.5),\n",
    "    \n",
    "    # Convert a PIL Image or numpy.ndarray to tensor\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Add some random gaussian noise to the image\n",
    "    transforms.Lambda(lambda x: x + (0.001**0.5)*torch.randn(x.shape)),\n",
    "    \n",
    "    # Normalize a tensor image with pre-computed mean and standard deviation\n",
    "    # (based on the data used to train the model(s))\n",
    "    # (be careful, it only works on torch.*Tensor)\n",
    "    transforms.Normalize(\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "        # mean=[0.485, 0.456, 0.406],\n",
    "        # std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define a different set of transforms testing\n",
    "# (for instance we do not need to flip the image)\n",
    "test_transform = transforms.Compose([\n",
    "    # transforms.Resize(100),\n",
    "    transforms.Resize((70, 210)),\n",
    "    # transforms.Grayscale(),\n",
    "    # transforms.CenterCrop(100),\n",
    "    # transforms.RandomCrop(100),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Mean and standard deviation were pre-computed on the training data\n",
    "    # (on the ImageNet dataset)\n",
    "    transforms.Normalize(\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "        # mean=[0.485, 0.456, 0.406],\n",
    "        # std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "# # Create a Dataset instance for our training data\n",
    "# data = TraversabilityDataset(\n",
    "#     traversal_costs_file=DATASET+\"traversal_costs.csv\",\n",
    "#     images_directory=DATASET+\"images\",\n",
    "#     transform=train_transform\n",
    "# )\n",
    "\n",
    "# # Split our training dataset into a training dataset and a validation dataset\n",
    "# train_set, val_set, test_set = random_split(data, [0.8, 0.1, 0.1])\n",
    "\n",
    "\n",
    "# Create a Dataset for training\n",
    "train_set = TraversabilityDataset(\n",
    "    traversal_costs_file=DATASET+\"traversal_costs_train.csv\",\n",
    "    images_directory=DATASET+\"images_train\",\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# Create a Dataset for validation\n",
    "val_set = TraversabilityDataset(\n",
    "    traversal_costs_file=DATASET+\"traversal_costs_train.csv\",\n",
    "    images_directory=DATASET+\"images_train\",\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "# Create a Dataset for testin\n",
    "test_set = TraversabilityDataset(\n",
    "    traversal_costs_file=DATASET+\"traversal_costs_test.csv\",\n",
    "    images_directory=DATASET+\"images_test\",\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "# Set the train dataset size\n",
    "# 70% of the total data is used for training, 15% for validation\n",
    "# and 15% for testing\n",
    "train_size = 70/(100-15)\n",
    "\n",
    "# Splits train data indices into train and validation data indices\n",
    "train_indices, val_indices = train_test_split(range(len(train_set)), train_size=train_size)\n",
    "\n",
    "# Extract the corresponding subsets of the train dataset\n",
    "train_set = Subset(train_set, train_indices)\n",
    "val_set = Subset(val_set, val_indices)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Combine a dataset and a sampler, and provide an iterable over the dataset\n",
    "# (setting shuffle argument to True calls a RandomSampler, and avoids to\n",
    "# have to create a Sampler object)\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=12,  # Asynchronous data loading and augmentation\n",
    "    pin_memory=True,  # Increase the transferring speed of the data to the GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # SequentialSampler\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of samples per split\n",
    "NB_TRAIN = len(train_set)\n",
    "NB_VAL = len(val_set)\n",
    "NB_TEST = len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0 % of the data for training (6239 samples)\n",
      "15.0 % of the data for validation (1337 samples)\n",
      "15.0 % of the data for testing (1338 samples)\n"
     ]
    }
   ],
   "source": [
    "# Display the splits ratio\n",
    "NB_SAMPLES = NB_TRAIN + NB_VAL + NB_TEST\n",
    "\n",
    "print(f\"{np.round(NB_TRAIN/NB_SAMPLES*100)} % of the data for training ({NB_TRAIN} samples)\")\n",
    "print(f\"{np.round(NB_VAL/NB_SAMPLES*100)} % of the data for validation ({NB_VAL} samples)\")\n",
    "print(f\"{np.round(NB_TEST/NB_SAMPLES*100)} % of the data for testing ({NB_TEST} samples)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a GPU if available\n",
    "# device = \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model design and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): ConvNormActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (2): ConvNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvNormActivation(\n",
      "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): ConvNormActivation(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Open TensorBoard\n",
    "tensorboard = SummaryWriter()\n",
    "\n",
    "# Load the pre-trained MobileNet model\n",
    "model = models.mobilenet_v2(pretrained=True).to(device=device)\n",
    "\n",
    "# Replace the last layer by a fully-connected one with 1 output\n",
    "model.classifier[1] = nn.Linear(model.last_channel, 1).to(device=device)\n",
    "\n",
    "# Initialize the last layer using Xavier initialization\n",
    "nn.init.xavier_uniform_(model.classifier[1].weight)\n",
    "\n",
    "# Display the architecture in TensorBoard\n",
    "images, traversal_costs, traversability_labels = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "tensorboard.add_graph(model, images)\n",
    "\n",
    "# Print the modified model\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 [train]: 100%|██████████| 195/195 [00:04<00:00, 45.79batch/s, batch_loss=0.106] \n",
      "Epoch 0 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.49batch/s, batch_loss=0.0507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.36281601312832956\n",
      "Validation loss:  0.1442838673080717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.33batch/s, batch_loss=0.0773]\n",
      "Epoch 1 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.68batch/s, batch_loss=0.149] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.11164980163941017\n",
      "Validation loss:  0.1061162543261335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.14batch/s, batch_loss=0.0582]\n",
      "Epoch 2 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.80batch/s, batch_loss=0.0595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.08871714281730163\n",
      "Validation loss:  0.0873978224893411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.88batch/s, batch_loss=0.0489]\n",
      "Epoch 3 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.16batch/s, batch_loss=0.0743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.07518534582012738\n",
      "Validation loss:  0.08451639479469686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.64batch/s, batch_loss=0.0499]\n",
      "Epoch 4 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.96batch/s, batch_loss=0.0828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.07015788298195753\n",
      "Validation loss:  0.07665552402890864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.70batch/s, batch_loss=0.0838]\n",
      "Epoch 5 [val]: 100%|██████████| 42/42 [00:00<00:00, 59.45batch/s, batch_loss=0.0567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.06576384671796591\n",
      "Validation loss:  0.0821743875387169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.23batch/s, batch_loss=0.0369]\n",
      "Epoch 6 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.10batch/s, batch_loss=0.106] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.06579391413774245\n",
      "Validation loss:  0.0690343396826869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [train]: 100%|██████████| 195/195 [00:04<00:00, 45.93batch/s, batch_loss=0.0668]\n",
      "Epoch 7 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.09batch/s, batch_loss=0.0517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.060101541781272645\n",
      "Validation loss:  0.059132762696771396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.15batch/s, batch_loss=0.0497]\n",
      "Epoch 8 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.47batch/s, batch_loss=0.0709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.059124393913990415\n",
      "Validation loss:  0.061727012551966165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.68batch/s, batch_loss=0.0909]\n",
      "Epoch 9 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.77batch/s, batch_loss=0.0403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05914178229868412\n",
      "Validation loss:  0.06325899729771274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.61batch/s, batch_loss=0.0838]\n",
      "Epoch 10 [val]: 100%|██████████| 42/42 [00:00<00:00, 59.55batch/s, batch_loss=0.0521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.057318581965489265\n",
      "Validation loss:  0.06351196969903651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.22batch/s, batch_loss=0.0873]\n",
      "Epoch 11 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.94batch/s, batch_loss=0.0595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.056959050740951145\n",
      "Validation loss:  0.061175567142310594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.22batch/s, batch_loss=0.0445]\n",
      "Epoch 12 [val]: 100%|██████████| 42/42 [00:00<00:00, 59.21batch/s, batch_loss=0.0424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.055620211477463065\n",
      "Validation loss:  0.062049913175758864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.60batch/s, batch_loss=0.045] \n",
      "Epoch 13 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.47batch/s, batch_loss=0.0655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05421142516036828\n",
      "Validation loss:  0.06154721718104113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.45batch/s, batch_loss=0.0385]\n",
      "Epoch 14 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.26batch/s, batch_loss=0.0767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.054212986859373556\n",
      "Validation loss:  0.0569526052900723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.63batch/s, batch_loss=0.0406]\n",
      "Epoch 15 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.95batch/s, batch_loss=0.0379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05440179174527144\n",
      "Validation loss:  0.05677450146703493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.13batch/s, batch_loss=0.039] \n",
      "Epoch 16 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.42batch/s, batch_loss=0.0532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05291803803008336\n",
      "Validation loss:  0.053251618962912334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.30batch/s, batch_loss=0.0697]\n",
      "Epoch 17 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.96batch/s, batch_loss=0.0387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05435436765352885\n",
      "Validation loss:  0.05672091152518988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.80batch/s, batch_loss=0.0341]\n",
      "Epoch 18 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.69batch/s, batch_loss=0.0799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05180748235911895\n",
      "Validation loss:  0.05093321683151381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [train]: 100%|██████████| 195/195 [00:04<00:00, 45.87batch/s, batch_loss=0.0386]\n",
      "Epoch 19 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.08batch/s, batch_loss=0.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05243951184436297\n",
      "Validation loss:  0.050275636393399464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 [train]: 100%|██████████| 195/195 [00:04<00:00, 45.49batch/s, batch_loss=0.0351]\n",
      "Epoch 20 [val]: 100%|██████████| 42/42 [00:00<00:00, 56.41batch/s, batch_loss=0.0389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05201527255181319\n",
      "Validation loss:  0.0562112404565726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.60batch/s, batch_loss=0.0554]\n",
      "Epoch 21 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.59batch/s, batch_loss=0.0358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.050541476828929705\n",
      "Validation loss:  0.05094274603539989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.81batch/s, batch_loss=0.0461]\n",
      "Epoch 22 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.97batch/s, batch_loss=0.0504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.049946146859572486\n",
      "Validation loss:  0.05348977952131203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.09batch/s, batch_loss=0.0326]\n",
      "Epoch 23 [val]: 100%|██████████| 42/42 [00:00<00:00, 56.43batch/s, batch_loss=0.037] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.049674129696228565\n",
      "Validation loss:  0.05303961406683638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.19batch/s, batch_loss=0.0406]\n",
      "Epoch 24 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.04batch/s, batch_loss=0.0862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.051326952998836835\n",
      "Validation loss:  0.05725682455868948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 [train]: 100%|██████████| 195/195 [00:04<00:00, 45.28batch/s, batch_loss=0.0656]\n",
      "Epoch 25 [val]: 100%|██████████| 42/42 [00:00<00:00, 59.31batch/s, batch_loss=0.051] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04966649106488778\n",
      "Validation loss:  0.050061402132823354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.40batch/s, batch_loss=0.065] \n",
      "Epoch 26 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.94batch/s, batch_loss=0.0337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05033729483301823\n",
      "Validation loss:  0.04884885721618221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.36batch/s, batch_loss=0.0236]\n",
      "Epoch 27 [val]: 100%|██████████| 42/42 [00:00<00:00, 59.41batch/s, batch_loss=0.0711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.047881896220720734\n",
      "Validation loss:  0.05073238217404911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.81batch/s, batch_loss=0.0335]\n",
      "Epoch 28 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.85batch/s, batch_loss=0.0287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.0491150197883447\n",
      "Validation loss:  0.050776259352763496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.06batch/s, batch_loss=0.0698]\n",
      "Epoch 29 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.71batch/s, batch_loss=0.0541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04882433259716401\n",
      "Validation loss:  0.05297428653353736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.22batch/s, batch_loss=0.0462]\n",
      "Epoch 30 [val]: 100%|██████████| 42/42 [00:00<00:00, 59.07batch/s, batch_loss=0.0425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04835145228948349\n",
      "Validation loss:  0.05205750762529317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.77batch/s, batch_loss=0.0698]\n",
      "Epoch 31 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.58batch/s, batch_loss=0.0512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04867420631150405\n",
      "Validation loss:  0.05237605928310326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.19batch/s, batch_loss=0.05]  \n",
      "Epoch 32 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.44batch/s, batch_loss=0.0311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04804013963693227\n",
      "Validation loss:  0.04928052984178066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.25batch/s, batch_loss=0.0223]\n",
      "Epoch 33 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.61batch/s, batch_loss=0.0355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.049218901179921934\n",
      "Validation loss:  0.05052381585396472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34 [train]: 100%|██████████| 195/195 [00:04<00:00, 45.86batch/s, batch_loss=0.0221]\n",
      "Epoch 34 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.47batch/s, batch_loss=0.0315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.047634182402338736\n",
      "Validation loss:  0.047338349744677544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.94batch/s, batch_loss=0.0401]\n",
      "Epoch 35 [val]: 100%|██████████| 42/42 [00:00<00:00, 55.91batch/s, batch_loss=0.0413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04796107676930916\n",
      "Validation loss:  0.04757003823206538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.14batch/s, batch_loss=0.0171]\n",
      "Epoch 36 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.52batch/s, batch_loss=0.0256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04624804722575041\n",
      "Validation loss:  0.047932034047941365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.11batch/s, batch_loss=0.0655]\n",
      "Epoch 37 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.50batch/s, batch_loss=0.0551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.046040873191295524\n",
      "Validation loss:  0.04968527287599586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.56batch/s, batch_loss=0.0462]\n",
      "Epoch 38 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.26batch/s, batch_loss=0.0588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.046732174089321725\n",
      "Validation loss:  0.051197112670966556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.18batch/s, batch_loss=0.0697]\n",
      "Epoch 39 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.71batch/s, batch_loss=0.0594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04561169065344028\n",
      "Validation loss:  0.04785453319726955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.10batch/s, batch_loss=0.0302]\n",
      "Epoch 40 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.38batch/s, batch_loss=0.0374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04649401809542607\n",
      "Validation loss:  0.045927832346586955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.53batch/s, batch_loss=0.0952]\n",
      "Epoch 41 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.99batch/s, batch_loss=0.0595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04606855667363375\n",
      "Validation loss:  0.04887339831995113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.63batch/s, batch_loss=0.0297]\n",
      "Epoch 42 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.44batch/s, batch_loss=0.0257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04541314494533417\n",
      "Validation loss:  0.04707181012435328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.20batch/s, batch_loss=0.0934]\n",
      "Epoch 43 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.62batch/s, batch_loss=0.0308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04543885320233993\n",
      "Validation loss:  0.04892560564691112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.13batch/s, batch_loss=0.0887]\n",
      "Epoch 44 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.47batch/s, batch_loss=0.0473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04507409908259526\n",
      "Validation loss:  0.05274895383488564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45 [train]: 100%|██████████| 195/195 [00:04<00:00, 47.24batch/s, batch_loss=0.0769]\n",
      "Epoch 45 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.86batch/s, batch_loss=0.0375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04785466541846593\n",
      "Validation loss:  0.049877678886765524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46 [train]: 100%|██████████| 195/195 [00:04<00:00, 45.48batch/s, batch_loss=0.0248]\n",
      "Epoch 46 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.84batch/s, batch_loss=0.0334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04550086959050252\n",
      "Validation loss:  0.05074301770045644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47 [train]: 100%|██████████| 195/195 [00:04<00:00, 45.56batch/s, batch_loss=0.0299]\n",
      "Epoch 47 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.71batch/s, batch_loss=0.0365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.044282891219243024\n",
      "Validation loss:  0.05002757454557078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.43batch/s, batch_loss=0.0697]\n",
      "Epoch 48 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.66batch/s, batch_loss=0.0359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.044929463788866995\n",
      "Validation loss:  0.046282230698991386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.36batch/s, batch_loss=0.0666]\n",
      "Epoch 49 [val]: 100%|██████████| 42/42 [00:00<00:00, 58.05batch/s, batch_loss=0.0295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04464751856449323\n",
      "Validation loss:  0.04664261337547075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50 [train]: 100%|██████████| 195/195 [00:04<00:00, 46.52batch/s, batch_loss=0.0313]\n",
      "Epoch 50 [val]: 100%|██████████| 42/42 [00:00<00:00, 57.70batch/s, batch_loss=0.0318]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04414912099735095\n",
      "Validation loss:  0.047767767017441135\n",
      "Early stopping at epoch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Get all the parameters excepts the weights and bias of fc layer\n",
    "base_params = [param for name, param in model.named_parameters()\n",
    "               if name not in [\"classifier.1.weight\", \"classifier.1.bias\"]]\n",
    "\n",
    "# Define the optimizer, with a greater learning rate for the new fc layer\n",
    "optimizer = optim.SGD([\n",
    "    {\"params\": base_params},\n",
    "    {\"params\": model.classifier.parameters(), \"lr\": 1e-3},\n",
    "],\n",
    "    lr=1e-3, momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "# An epoch is one complete pass of the training dataset through the network\n",
    "NB_EPOCHS = 100\n",
    "\n",
    "# Number of epochs we wait for the loss to decrease before stopping\n",
    "# the training process early\n",
    "patience = 10\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "loss_values = torch.zeros(2, NB_EPOCHS)\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    \n",
    "    # Training\n",
    "    train_loss = 0.\n",
    "    \n",
    "    # Configure the model for training\n",
    "    # (good practice, only necessary if the model operates differently for\n",
    "    # training and validation)\n",
    "    model.train()\n",
    "    \n",
    "    # Add a progress bar\n",
    "    train_loader_pbar = tqdm(train_loader, unit=\"batch\")\n",
    "    \n",
    "    # Loop over the training batches\n",
    "    for images, traversal_costs, _ in train_loader_pbar:\n",
    "        \n",
    "        # Print the epoch and training mode\n",
    "        train_loader_pbar.set_description(f\"Epoch {epoch} [train]\")\n",
    "        \n",
    "        # Move images and traversal scores to GPU (if available)\n",
    "        images = images.to(device)\n",
    "        traversal_costs = traversal_costs.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        # Zero out gradients before each backpropagation pass, to avoid that\n",
    "        # they accumulate\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        predicted_traversal_costs = model(images)\n",
    "        \n",
    "        # Compute loss \n",
    "        loss = criterion(predicted_traversal_costs[:, 0], traversal_costs)\n",
    "        \n",
    "        # Print the batch loss next to the progress bar\n",
    "        train_loader_pbar.set_postfix(batch_loss=loss.item())\n",
    "        \n",
    "        # Perform backpropagation (update weights)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Adjust parameters based on gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate batch loss to average over the epoch\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    # Validation\n",
    "    val_loss = 0.\n",
    "    \n",
    "    # Configure the model for testing\n",
    "    # (turn off dropout layers, batchnorm layers, etc)\n",
    "    model.eval()\n",
    "    \n",
    "    # Add a progress bar\n",
    "    val_loader_pbar = tqdm(val_loader, unit=\"batch\")\n",
    "    \n",
    "    # Turn off gradients computation (the backward computational graph is built during\n",
    "    # the forward pass and weights are updated during the backward pass, here we avoid\n",
    "    # building the graph)\n",
    "    with torch.no_grad():\n",
    "        # Loop over the validation batches\n",
    "        for images, traversal_costs, _ in val_loader_pbar:\n",
    "\n",
    "            # Print the epoch and validation mode\n",
    "            val_loader_pbar.set_description(f\"Epoch {epoch} [val]\")\n",
    "\n",
    "            # Move images and traversal scores to GPU (if available)\n",
    "            images = images.to(device)\n",
    "            traversal_costs = traversal_costs.type(torch.FloatTensor).to(device)\n",
    "            \n",
    "            # Perform forward pass (only, no backpropagation)\n",
    "            predicted_traversal_costs = model(images)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predicted_traversal_costs[:, 0], traversal_costs)\n",
    "            # Print the batch loss next to the progress bar\n",
    "            val_loader_pbar.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "            # Accumulate batch loss to average over the epoch\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    \n",
    "    # Compute the losses\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(\"Train loss: \", train_loss)\n",
    "    print(\"Validation loss: \", val_loss)\n",
    "    \n",
    "    # Store the computed losses\n",
    "    loss_values[0, epoch] = train_loss\n",
    "    loss_values[1, epoch] = val_loss\n",
    "    \n",
    "    # Add the losses to TensorBoard\n",
    "    tensorboard.add_scalar(\"train_loss\", train_loss, epoch)\n",
    "    tensorboard.add_scalar(\"val_loss\", val_loss, epoch)\n",
    "    \n",
    "    # Early stopping based on validation loss: stop the training if the\n",
    "    # loss has not improved for the last 5 epochs\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    elif epoch - best_epoch >= patience:\n",
    "        print(f'Early stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "# Close TensorBoard\n",
    "tensorboard.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['text.usetex'] = False\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss\n",
    "indices = loss_values[0] != 0\n",
    "train_losses = loss_values[0][indices]\n",
    "val_losses = loss_values[1][indices]\n",
    "\n",
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(val_losses, label=\"validation loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Losses (MSE)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.04919623183883936\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_loss = 0.\n",
    "\n",
    "# Configure the model for testing\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Loop over the testing batches\n",
    "    for images, traversal_costs, _ in test_loader:\n",
    "        \n",
    "        images = images.to(device)\n",
    "        traversal_costs = traversal_costs.to(device)\n",
    "        \n",
    "        # Perform forward pass\n",
    "        predicted_traversal_costs = model(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predicted_traversal_costs[:, 0], traversal_costs)\n",
    "        \n",
    "        # Accumulate batch loss to average of the entire testing set\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "# Compute the loss and accuracy\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "print(\"Test loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " tensor([[1.9067],\n",
      "        [1.3779],\n",
      "        [1.8289],\n",
      "        [1.8622],\n",
      "        [1.8467],\n",
      "        [1.6435],\n",
      "        [1.7196],\n",
      "        [1.7600],\n",
      "        [1.7562],\n",
      "        [1.8696],\n",
      "        [1.4030],\n",
      "        [1.4746],\n",
      "        [1.8216],\n",
      "        [1.8370],\n",
      "        [1.5567],\n",
      "        [1.9565],\n",
      "        [1.5578],\n",
      "        [1.7571],\n",
      "        [1.9623],\n",
      "        [1.4862],\n",
      "        [1.8262],\n",
      "        [1.6685],\n",
      "        [1.4711],\n",
      "        [1.8584],\n",
      "        [1.7102],\n",
      "        [1.4451]], device='cuda:0')\n",
      "Ground truth:\n",
      " tensor([1.4431, 1.8142, 1.4213, 1.8988, 1.6864, 1.0896, 1.3164, 1.1830, 1.9168,\n",
      "        1.0740, 1.7578, 1.9497, 1.1054, 1.4927, 2.1730, 1.6349, 1.9143, 1.6020,\n",
      "        1.8331, 2.0072, 1.7344, 1.1041, 1.8847, 1.4045, 0.9980, 2.2835, 1.8514,\n",
      "        1.1247, 2.0487, 1.2955, 1.9082, 1.9524], device='cuda:0',\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "images, traversal_costs, _ = next(iter(test_loader))\n",
    "\n",
    "images = images.to(device)\n",
    "traversal_costs = traversal_costs.to(device)\n",
    "\n",
    "predicted_traversal_scores = model(images)\n",
    "# predicted_traversal_scores = nn.Softmax(dim=1)(model(images))\n",
    "\n",
    "print(\"Output:\\n\", predicted_traversal_costs)\n",
    "print(\"Ground truth:\\n\", traversal_costs)\n",
    "\n",
    "# print(predicted_traversal_scores-traversal_scores)\n",
    "\n",
    "# predicted_traversal_scores = predicted_traversal_scores.to(\"cpu\").detach().numpy()\n",
    "# plt.hist(predicted_traversal_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traversal_costs_train = []\n",
    "\n",
    "for _, score, _ in train_set:\n",
    "    traversal_costs_train.append(score)\n",
    "    \n",
    "plt.hist(traversal_costs_train, bins=10)\n",
    "plt.xlabel(\"Traversal Cost\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of Traversal Costs (Training Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the model parameters\n",
    "torch.save(model.state_dict(), \"v2_mobilenet.params\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b41780248731413e84842f51b5f7cba9458d17cab322c1e6e009edd67950520d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
